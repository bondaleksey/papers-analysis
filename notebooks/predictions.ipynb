{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the year from the abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import string # библиотека для работы со строками\n",
    "import nltk   # Natural Language Toolkit\n",
    "# загружаем библиотеку для лемматизации\n",
    "import pymorphy2 # Морфологический анализатор\n",
    "\n",
    "#вычисляем tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_types import AuthorsDB\n",
    "from data_types import PublicationsDB\n",
    "from data_types import AbstractsDB\n",
    "\n",
    "audb = AuthorsDB()\n",
    "audb.load()\n",
    "pubdb = PublicationsDB()\n",
    "pubdb.load()\n",
    "absdb = AbstractsDB()\n",
    "absdb.load()\n",
    "\n",
    "filename = \"../data/mnid_author_dict.pkl\"\n",
    "with open(filename,'rb') as inp:\n",
    "    authors_dict = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors_dict2 = {v['mn_id']:v['name'] for k,v in authors_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "for k, v in authors_dict.items():\n",
    "    ind += 1\n",
    "    print(k,v)\n",
    "    if ind>4:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = '../data/mnid_author_dict.pkl'    \n",
    "# with open(filename,'wb') as outp:\n",
    "#     pickle.dump(authors_dict2, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"../data/mathnet_iam_authors_dict.pkl\"\n",
    "# with open(filename,'rb') as inp:\n",
    "#     authors_dict = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub = pd.DataFrame.from_dict(pubdb.db,orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to get \n",
    "- abstract and it's tokenization\n",
    "- year of paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = absdb.db[absdb.db['abstract'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absdb.db.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем список стоп-слов для русского\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "# примеры стоп-слов\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# знаки препинания\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dates = [str(x) for x in np.arange(1900, 2022)]\n",
    "\n",
    "\n",
    "def process_abstractsdb(data):\n",
    "    texts = []    \n",
    "    \n",
    "    # поочередно проходим по всем новостям в списке\n",
    "    for index,row in data.iterrows():\n",
    "        # print(row['abstract'])        \n",
    "        # print(type(row['abstract']))\n",
    "        text = row['abstract']\n",
    "        if (row['keywords'] is not None):\n",
    "            # print(text)\n",
    "            # print(row['keywords'])\n",
    "            # print(type(row['keywords']))\n",
    "            text += row['keywords']                   \n",
    "        text_lower = text.lower() # приводим все слова к нижнему регистру\n",
    "        tokens     = word_tokenizer.tokenize(text_lower) #разбиваем екст на слова                \n",
    "        tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words and not word.isnumeric())]\n",
    "                      \n",
    "        texts.append(tokens) # добавляем в предобработанный список\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts.loc[['vyurv213','vyurv1','vyurv46']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = process_abstractsdb(abstracts.loc[['vyurv213','vyurv1','vyurv46']])\n",
    "# texts = process_abstractsdb(abstracts.loc[['vyurv213','vyurv1','vyurv46']])\n",
    "texts = process_abstractsdb(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем лемматизатор :)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for aword in texts[2]:\n",
    "    aword_norm = morph.parse(aword)[0].normal_form\n",
    "    print(\"Исходное слово: %s \\tЛемматизированное: %s\" % (aword, aword_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применяем лемматизацию ко всем текстам\n",
    "print(range(len(texts)))\n",
    "\n",
    "file_lemmatized_text = '../data/lemmatized_text.pkl'\n",
    "if exists(file_lemmatized_text):    \n",
    "    with open(file_lemmatized_text,'rb') as inp:\n",
    "        texts = pickle.load(inp)\n",
    "else:\n",
    "    for i in tqdm(range(len(texts))):           # tqdm_notebook создает шкалу прогресса :)\n",
    "        text_lemmatized = [morph.parse(x)[0].normal_form for x in texts[i]] # применяем лемматизацию для каждого слова в тексте\n",
    "        texts[i] = ' '.join(text_lemmatized)\n",
    "    # time.sleep(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lemmatized_text(data, status='write'):\n",
    "    filename = '../data/lemmatized_text.pkl'    \n",
    "    if status == 'write':        \n",
    "        with open(filename,'wb') as outp:\n",
    "            pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)\n",
    "# save_lemmatized_text(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/46118910/scikit-learn-vectorizer-max-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit TF-IDF on train texts\n",
    "# vectorizer = TfidfVectorizer(min_df=1,   max_features=None, strip_accents='unicode',  analyzer='word',token_pattern=r'\\w{2,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)\n",
    "# vectorizer = TfidfVectorizer(min_df=1,   max_features=None, analyzer='word',token_pattern=r'\\w{2,}', ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1) \n",
    "my_min_df = 0.0002\n",
    "vectorizer_uni = TfidfVectorizer(min_df=1,   max_features=None, analyzer='word',token_pattern=r'\\w{2,}', ngram_range=(1, 1), use_idf=1,smooth_idf=1,sublinear_tf=1) \n",
    "vectorizer_bi = TfidfVectorizer(min_df=my_min_df,   max_features=None, analyzer='word',token_pattern=r'\\w{2,}', ngram_range=(2, 2), use_idf=1,smooth_idf=1,sublinear_tf=1) \n",
    "vectorizer_tri = TfidfVectorizer(min_df=my_min_df,   max_features=None, analyzer='word',token_pattern=r'\\w{2,}', ngram_range=(3, 3), use_idf=1,smooth_idf=1,sublinear_tf=1) \n",
    "vectorizer_uni.fit(texts)\n",
    "vectorizer_bi.fit(texts)\n",
    "vectorizer_tri.fit(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('uni:', len(vectorizer_uni.get_feature_names_out()))\n",
    "print('bi:',len(vectorizer_bi.get_feature_names_out()))\n",
    "print('tri:',len(vectorizer_tri.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 100\n",
    "for ind,item in enumerate(vectorizer_uni.get_feature_names_out()):\n",
    "    print(ind, item)\n",
    "    if ind >max_val:\n",
    "        break\n",
    "\n",
    "for ind,item in enumerate(vectorizer_bi.get_feature_names_out()):\n",
    "    print(ind, item)\n",
    "    if ind >max_val:\n",
    "        break\n",
    "    \n",
    "for ind,item in enumerate(vectorizer_tri.get_feature_names_out()):\n",
    "    print(ind, item)\n",
    "    if ind >max_val:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix_uni = vectorizer_uni.fit_transform(texts)\n",
    "tf_idf_matrix_bi = vectorizer_bi.fit_transform(texts)\n",
    "tf_idf_matrix_tri = vectorizer_tri.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tf_idf_matrix_uni))\n",
    "print(tf_idf_matrix_uni.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectorizer(data,name,status='write'):\n",
    "    filename = '../data/'+name+'.pkl'    \n",
    "    if status == 'write':        \n",
    "        with open(filename,'wb') as outp:\n",
    "            pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "def save_tf_idf_matrix(data,name,status='write'):\n",
    "    filename = '../data/'+name+'.pkl'    \n",
    "    if status == 'write':        \n",
    "        with open(filename,'wb') as outp:\n",
    "            pickle.dump(data, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_vectorizer(vectorizer_uni,'vectorizer_uni',status=\"write\")\n",
    "# save_vectorizer(vectorizer_bi,'vectorizer_bi',status=\"write\")\n",
    "# save_vectorizer(vectorizer_tri,'vectorizer_tri',status=\"write\")\n",
    "# save_tf_idf_matrix(tf_idf_matrix_uni,'tf_idf_matrix_uni',status=\"write\")\n",
    "# save_tf_idf_matrix(tf_idf_matrix_bi,'tf_idf_matrix_bi',status=\"write\")\n",
    "# save_tf_idf_matrix(tf_idf_matrix_tri,'tf_idf_matrix_tri',status=\"write\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer(name):\n",
    "    filename = '../data/'+name+'.pkl'\n",
    "    res = None    \n",
    "    with open(filename,'rb') as inp:\n",
    "        res = pickle.load(inp)\n",
    "    return res\n",
    "\n",
    "def load_tf_idf_matrix(name):\n",
    "    filename = '../data/'+name+'.pkl'\n",
    "    res = None    \n",
    "    with open(filename,'rb') as inp:\n",
    "        res = pickle.load(inp)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = load_vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of the article for Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_request(data):        \n",
    "    text_lower = data.lower()\n",
    "    tokens = word_tokenizer.tokenize(text_lower)\n",
    "    tokens = [word for word in tokens if (word not in string.punctuation and word not in stop_words and not word.isnumeric())]        \n",
    "    texts = ' '.join(tokens)         \n",
    "    return [morph.parse(texts)[0].normal_form]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_authors_rating(df, coef):\n",
    "    # print(df)\n",
    "    # print(type(df))\n",
    "    # print(coef)\n",
    "    # print(type(coef))\n",
    "    rating = defaultdict(int)\n",
    "    \n",
    "    for index in range(len(df)):\n",
    "        # print(f'index = {index}, row = {df.iloc[index]}')        \n",
    "        for author in df.iloc[index]['author_id']:  \n",
    "            # print(index)          \n",
    "            # print(f'add coef = {coef[index]} to author = {author}')\n",
    "            rating[author] += coef[index]            \n",
    "            # print(rating)\n",
    "    # dict(sorted(rating.items(), key=lambda item: item[1]))\n",
    "    rating_list = [ (k,v) for k, v in sorted(rating.items(), key=lambda item: item[1], reverse=True)]\n",
    "    return rating_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# request = process_request(request)\n",
    "# print(request)\n",
    "# print(type(request))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)\n",
    "def get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict):    \n",
    "    request = process_request(request)    \n",
    "    vect_req = vectorizer.transform(request)\n",
    "    onehot = Binarizer()\n",
    "    ohe_request = onehot.fit_transform(vect_req.toarray())    \n",
    "    coef= tf_idf_matrix.dot(ohe_request.T)    \n",
    "    coef = coef.flatten()    \n",
    "    \n",
    "    max_number_of_articles = 30\n",
    "    low_bound = 0.01\n",
    "    max_output_pubs = 5\n",
    "    max_output_authors = 5\n",
    "    \n",
    "    pubs_index = np.argsort(coef)[::-1][:max_number_of_articles]        \n",
    "    low_bound_pub_index  = [ind for ind in pubs_index if coef[ind]>low_bound]\n",
    "    req_pups = list(abstracts.iloc[low_bound_pub_index].index)    \n",
    "    top_pubs = pub.loc[req_pups]['reference']\n",
    "    top_autors = get_authors_rating(pub.loc[req_pups],coef[pubs_index])\n",
    "        \n",
    "    index = 0\n",
    "    for item in top_autors:        \n",
    "        if index >= max_output_authors:\n",
    "            break\n",
    "        if item[0] in authors_dict:\n",
    "            index +=1\n",
    "            print(f'author: {authors_dict[item[0]]}, raiting: {round(item[1],2)}')            \n",
    "            \n",
    "    for item in top_pubs[:max_output_pubs]:\n",
    "        print(item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_response_print(request, vectorizer, tf_idf_matrix, abstracts, authors_dict):\n",
    "    # tf_idf_matrix= train_X = vectorizer.fit_transform(texts)\n",
    "    # print(request)\n",
    "    # print(type(request))\n",
    "    request = process_request(request)\n",
    "    # print(request)\n",
    "    # print(type(request))\n",
    "    vect_req = vectorizer.transform(request)\n",
    "    onehot = Binarizer()\n",
    "    ohe_request = onehot.fit_transform(vect_req.toarray())\n",
    "    # print(ohe_request.shape)\n",
    "    # print(tf_idf_matrix.shape)\n",
    "    coef= tf_idf_matrix.dot(ohe_request.T)\n",
    "    # print(coef)\n",
    "    coef = coef.flatten()\n",
    "    \n",
    "    \n",
    "    max_number_of_articles = 30\n",
    "    low_bound = 0.01\n",
    "    max_output_pubs = 5\n",
    "    max_output_authors = 5\n",
    "    \n",
    "    pubs_index = np.argsort(coef)[::-1][:max_number_of_articles]\n",
    "    # print(pubs_index)\n",
    "    # coef[pubs_index]\n",
    "    \n",
    "    low_bound_pub_index  = [ind for ind in pubs_index if coef[ind]>low_bound]\n",
    "    # print(low_bound_pub_index)\n",
    "    # print(coef[low_bound_pub_index])\n",
    "    \n",
    "    req_pups = list(abstracts.iloc[low_bound_pub_index].index)\n",
    "    print(req_pups)\n",
    "    print(pub.loc[req_pups].head())\n",
    "    \n",
    "    top_pubs = pub.loc[req_pups]['reference']\n",
    "    \n",
    "    top_autors = get_authors_rating(pub.loc[req_pups],coef[pubs_index])\n",
    "    print(top_autors[:max_output_authors][0], top_autors[:max_output_authors][1])\n",
    "    print(type(top_autors))\n",
    "    print(top_autors)\n",
    "    index = 0\n",
    "    for item in top_autors:\n",
    "        print(index, item)        \n",
    "        if index >= max_output_authors:\n",
    "            break\n",
    "        if item[0] in authors_dict:\n",
    "            index +=1\n",
    "            print(f'author = {authors_dict[item[0]]}, his raiting = {round(item[1],2)}')\n",
    "            \n",
    "            \n",
    "    for item in top_pubs[:max_output_pubs]:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'суперкомпьютерное моделирование отказоустойчивые вычисления на hpc системах'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'суперкомпьютерное'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'моделирование'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'отказоустойчивые вычисления на hpc системах'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'отказоустойчивые вычисления'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'отказоустойчивые'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'ULFM'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'вычисления'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'интеллект'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'искусственный'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = 'искусственный интеллект'\n",
    "result =  get_model_response(request, vectorizer, tf_idf_matrix, abstracts, authors_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5d6f671cb4cf511d2ed9bdc94976a7d6a33f5397885a0aeec498474d8c496680"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tbot4papersanalysis.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
